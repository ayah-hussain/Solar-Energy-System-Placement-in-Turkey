{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bae736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "from shapely.geometry import Point\n",
    "from math import radians, cos, sin, sqrt, atan2\n",
    "from fastkml import kml\n",
    "import geopandas as gpd\n",
    "from pykml.factory import KML_ElementMaker as KML\n",
    "from lxml import etree\n",
    "from shapely.geometry import Polygon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09251cb7",
   "metadata": {},
   "source": [
    "# Fetching Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51faba81",
   "metadata": {},
   "source": [
    "### The following cells are commented because fetching the data took about 4 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb27c6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def fetch_nasa_power_data_for_point(lat, lon, start=\"2000-01-01\", end=\"2024-12-31\"):\n",
    "    parameters = [\n",
    "        \"ALLSKY_SFC_SW_DWN\",   # Surface Shortwave Downward Irradiance\n",
    "        \"ALLSKY_KT\",           # Clearness Index\n",
    "        \"CLOUD_AMT\",           # Cloud Amount\n",
    "        \"PRECTOTCORR\"          # Precipitation\n",
    "    ]\n",
    "\n",
    "    url = (\n",
    "        f\"https://power.larc.nasa.gov/api/temporal/monthly/point\"\n",
    "        f\"?start={start.replace('-', '')}&end={end.replace('-', '')}\"\n",
    "        f\"&latitude={lat}&longitude={lon}\"\n",
    "        f\"&community=AG\"\n",
    "        f\"&parameters={','.join(parameters)}\"\n",
    "        f\"&format=JSON\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        records = data[\"properties\"][\"parameter\"]\n",
    "        dates = list(records[parameters[0]].keys())\n",
    "\n",
    "        results = []\n",
    "        for date in dates:\n",
    "            entry = {\"lat\": lat, \"lon\": lon, \"date\": date}\n",
    "            for param in parameters:\n",
    "                entry[param] = records[param][date]\n",
    "            results.append(entry)\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for ({lat}, {lon}): {e}\")\n",
    "        return pd.DataFrame()\n",
    "   \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389f1d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "df = fetch_nasa_power_data_for_point(42.002, 28.254, start=\"2000\", end=\"2024\")\n",
    "print(df.head())\n",
    "df.to_csv(\"nasa_test_point.csv\", index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38fb8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "df = pd.read_csv(\"assets/turkey_grid.csv\")\n",
    "\n",
    "batch_size = 200\n",
    "total_points = len(df)\n",
    "\n",
    "output_dir = \"assets/nasa_datas\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for batch_start in range(0, total_points, batch_size):\n",
    "    batch_end = min(batch_start + batch_size, total_points)\n",
    "    batch = df.iloc[batch_start:batch_end]\n",
    "\n",
    "    all_results = []\n",
    "    print(f\"\\nüîÑ Processing block {batch_start}‚Äì{batch_end - 1}\")\n",
    "\n",
    "    for i, row in batch.iterrows():\n",
    "        lat, lon = row[\"lat\"], row[\"lon\"]\n",
    "        print(f\"‚Üí Fetching point {i+1}/{total_points}: ({lat}, {lon})\")\n",
    "        \n",
    "        data = fetch_nasa_power_data_for_point(lat, lon, start=\"2020\", end=\"2023\")\n",
    "        if not data.empty:\n",
    "            all_results.append(data)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    if all_results:\n",
    "        block_df = pd.concat(all_results, ignore_index=True)\n",
    "        block_index = (batch_start // batch_size) + 1\n",
    "        output_file = os.path.join(output_dir, f\"nasa_block_{block_index}.csv\")\n",
    "        block_df.to_csv(output_file, index=False)\n",
    "        print(f\"‚úÖ Saved block to {output_file}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No data collected for block {batch_start}‚Äì{batch_end - 1}\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1343a853",
   "metadata": {},
   "source": [
    "# Sorting and Merging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b1ecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_start, lat_end = 35, 43\n",
    "lon_start, lon_end = 25, 46\n",
    "lat_step, lon_step = 1, 1\n",
    "\n",
    "lat_points = [round(lat_start + i * lat_step, 4) for i in range(int((lat_end - lat_start) / lat_step) + 1)]\n",
    "lon_points = [round(lon_start + i * lon_step, 4) for i in range(int((lon_end - lon_start) / lon_step) + 1)]\n",
    "\n",
    "grid_points = [(lat, lon) for lat in lat_points for lon in lon_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99db3569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kml(gdf, filename):\n",
    "    placemarks = []\n",
    "    for _, row in gdf.iterrows():\n",
    "        lon, lat = row.geometry.x, row.geometry.y\n",
    "        placemarks.append(\n",
    "            KML.Placemark(\n",
    "                KML.name(f\"{lat}, {lon}\"),\n",
    "                KML.Point(KML.coordinates(f\"{lon},{lat},0\"))\n",
    "            )\n",
    "        )\n",
    "\n",
    "    kml_doc = KML.kml(KML.Document(*placemarks))\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(etree.tostring(kml_doc, pretty_print=True).decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c339278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "geoms = [Point(lon, lat) for lat, lon in grid_points]\n",
    "gdf = gpd.GeoDataFrame(geometry=geoms, crs=\"EPSG:4326\")\n",
    "\n",
    "create_kml(gdf, \"assets/turkey_grid.kml\")\n",
    "\n",
    "gdf[\"lon\"] = gdf.geometry.x\n",
    "gdf[\"lat\"] = gdf.geometry.y\n",
    "gdf[[\"lat\", \"lon\"]].to_csv(\"assets/turkey_grid.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6d1d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "polygon_df = pd.read_csv(\"assets/Turkey.csv\")\n",
    "polygon_list = []\n",
    "for row in polygon_df.iloc[:, 0]:\n",
    "    coords = [(float(p.split(\",\")[0]), float(p.split(\",\")[1])) for p in row.strip().split()]\n",
    "    polygon_list.append(Polygon(coords))\n",
    "\n",
    "gdf[\"is_inside\"] = gdf.geometry.apply(lambda point: any(point.within(poly) for poly in polygon_list))\n",
    "\n",
    "inside_points = gdf[gdf[\"is_inside\"]].copy()\n",
    "\n",
    "# KML\n",
    "create_kml(inside_points, \"assets/Turkey_filtered_grid.kml\")\n",
    "\n",
    "# CSV\n",
    "inside_points[\"lon\"] = inside_points.geometry.x\n",
    "inside_points[\"lat\"] = inside_points.geometry.y\n",
    "inside_points[[\"lat\", \"lon\"]].to_csv(\"assets/istanbul_filtered_grid.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f94400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lat_lon = pd.read_csv('assets/nasa_datas_sorted/merged_all.csv')\n",
    "geoms = [Point(lon, lat) for lat, lon in zip(df_lat_lon['lat'], df_lat_lon['lon'])]\n",
    "gdf = gpd.GeoDataFrame(geometry=geoms, crs=\"EPSG:4326\")\n",
    "\n",
    "gdf['lon'] = gdf.geometry.x\n",
    "gdf['lat'] = gdf.geometry.y\n",
    "\n",
    "polygon_df = pd.read_csv(\"assets/Turkey.csv\")\n",
    "polygon_list = []\n",
    "for row in polygon_df.iloc[:, 0]:\n",
    "    coords = [(float(p.split(\",\")[0]), float(p.split(\",\")[1])) for p in row.strip().split()]\n",
    "    polygon_list.append(Polygon(coords))\n",
    "\n",
    "gdf[\"is_inside\"] = gdf.geometry.apply(lambda point: any(point.within(poly) for poly in polygon_list))\n",
    "\n",
    "inside_points = gdf[gdf[\"is_inside\"]].copy()\n",
    "\n",
    "# KML\n",
    "create_kml(inside_points, \"assets/Turkey_filtered_grid.kml\")\n",
    "\n",
    "# CSV\n",
    "inside_points[\"lon\"] = inside_points.geometry.x\n",
    "inside_points[\"lat\"] = inside_points.geometry.y\n",
    "inside_points[[\"lat\", \"lon\"]].to_csv(\"assets/turkey_filtered_grid.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053276b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'assets/nasa_datas'\n",
    "output_dir = 'assets/nasa_datas_sorted'\n",
    "output_file = os.path.join(output_dir, 'nasa_original_combined.csv')\n",
    "parameters = ['ALLSKY_SFC_SW_DWN', 'ALLSKY_KT', 'CLOUD_AMT', 'PRECTOTCORR']\n",
    "transformed_dfs = []\n",
    "for i in range(1, 2):\n",
    "    filename = f'nasa_block_{i}.csv'\n",
    "    filepath = os.path.join(input_dir, filename)\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "    pivot_dfs = []\n",
    "    for param in parameters:\n",
    "        pivot = df.pivot_table(index=['lat', 'lon'], columns='date', values=param)\n",
    "        pivot.columns = [f\"{param}_{col}\" for col in pivot.columns]\n",
    "        pivot_dfs.append(pivot)\n",
    "\n",
    "    final_df = pd.concat(pivot_dfs, axis=1).reset_index()\n",
    "    transformed_dfs.append(final_df)\n",
    "\n",
    "combined_df = pd.concat(transformed_dfs, axis=0, ignore_index=True)\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "combined_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c826329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.read_csv('assets/nasa_datas_sorted/nasa_original_combined.csv')\n",
    "k = kml.KML()\n",
    "doc = kml.Document(id='docid', name='Merged Points')\n",
    "\n",
    "folder_original = kml.Folder(id='original_folder')\n",
    "\n",
    "style_original = \"\"\"\n",
    "<Style id=\"original_style\">\n",
    "  <IconStyle>\n",
    "    <color>ff0000ff</color>\n",
    "    <scale>1.2</scale>\n",
    "    <Icon>\n",
    "      <href>http://maps.google.com/mapfiles/kml/paddle/blu-circle.png</href>\n",
    "    </Icon>\n",
    "  </IconStyle>\n",
    "</Style>\n",
    "\"\"\"\n",
    "\n",
    "for _, row in df_merged.iterrows():\n",
    "    point = Point(float(row['lon']), float(row['lat']))\n",
    "    name = f\"{round(row['lat'], 3)}, {round(row['lon'], 3)}\"\n",
    "    placemark = kml.Placemark(id=None, name=name, description=None, geometry=point)\n",
    "    placemark.styleUrl = '#original_style'\n",
    "    folder_original.append(placemark)\n",
    "        \n",
    "doc.append(folder_original)\n",
    "k.append(doc)\n",
    "\n",
    "kml_string = k.to_string(prettyprint=True)\n",
    "kml_string = kml_string.replace('</Document>', f'{style_original}</Document>')\n",
    "\n",
    "with open('assets/nasa_datas_sorted/merged_all.kml', 'w', encoding='utf-8') as f:\n",
    "    f.write(kml_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356eb542",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grid = pd.read_csv('assets/turkey_filtered_grid.csv')\n",
    "df_merged = pd.read_csv('assets/nasa_datas_sorted/merged_all.csv')\n",
    "df_filtered = df_merged.merge(df_grid, on=['lat', 'lon'], how='inner')\n",
    "df_filtered.to_csv('merged_all_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0449f2e1",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44ac6d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your DataFrame (example)\n",
    "df = pd.read_csv(\"merged_all_filtered.csv\")\n",
    "\n",
    "# Drop columns that contain \"13\" which is the average of the year column\n",
    "df_cleaned = df.loc[:, ~df.columns.str.contains(\"13\")]\n",
    "\n",
    "# Save to a new file\n",
    "df_cleaned.to_csv(\"merged_all_filtered_no13.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47d4abb",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5572a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your DataFrame\n",
    "df = pd.read_csv(\"merged_all_filtered_no13.csv\")\n",
    "\n",
    "# Base columns to normalize\n",
    "base_columns = ['ALLSKY_SFC_SW_DWN','ALLSKY_KT', 'CLOUD_AMT', 'PRECTOTCORR']\n",
    "\n",
    "normalized_columns = {\n",
    "    'lat': df['lat'],\n",
    "    'lon': df['lon']\n",
    "}\n",
    "\n",
    "# Normalize matching monthly columns\n",
    "for col_base in base_columns:\n",
    "    pattern = re.compile(rf'^{col_base}_\\d{{6}}$')  # Matches e.g. ALLSKY_SFC_SW_DWN_202001\n",
    "    matching_cols = [col for col in df.columns if pattern.match(col)]\n",
    "\n",
    "    for col in matching_cols:\n",
    "        min_val = df[col].min()\n",
    "        max_val = df[col].max()\n",
    "        if max_val - min_val != 0:\n",
    "            normalized_columns[col] = ((df[col] - min_val) / (max_val - min_val)).round(5)\n",
    "        else:\n",
    "            normalized_columns[col] = df[col].round(5)\n",
    "\n",
    "# Create the DataFrame\n",
    "normalized_df = pd.DataFrame(normalized_columns)\n",
    "\n",
    "# Save to CSV\n",
    "normalized_df.to_csv(\"all_data_normalized.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2da2ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALLSKY_SFC_SW_DWN ‚Üí 27156 unique values across months\n",
      "ALLSKY_KT ‚Üí 3889 unique values across months\n",
      "CLOUD_AMT ‚Üí 28685 unique values across months\n",
      "PRECTOTCORR ‚Üí 22741 unique values across months\n"
     ]
    }
   ],
   "source": [
    "# Data analysis\n",
    "features = ['ALLSKY_SFC_SW_DWN', 'ALLSKY_KT', 'CLOUD_AMT', 'PRECTOTCORR']\n",
    "unique_counts = {}\n",
    "df = pd.read_csv(\"all_data_normalized.csv\")\n",
    "for feature in features:\n",
    "    # Get all monthly columns for this feature\n",
    "    cols = [col for col in df.columns if col.startswith(feature)]\n",
    "    \n",
    "    # Concatenate all values into one Series\n",
    "    combined = pd.concat([df[col] for col in cols])\n",
    "    \n",
    "    # Count unique values across all months\n",
    "    unique_counts[feature] = combined.nunique()\n",
    "\n",
    "# Show result\n",
    "for feat, count in unique_counts.items():\n",
    "    print(f\"{feat} ‚Üí {count} unique values across months\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd7f4fe",
   "metadata": {},
   "source": [
    "## Calculate Monthly scores for each point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15d09818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'monthly_suitability_scores_with_avg.csv'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"all_data_normalized.csv\")\n",
    "\n",
    "# Define the suitability function \n",
    "def calculate_suitability(row, w0=0.8, w1=0.3, w2=-0.2, w3=-0.2):\n",
    "    return (\n",
    "        w0 * row['ALLSKY_SFC_SW_DWN'] +\n",
    "        w1 * row['ALLSKY_KT'] +\n",
    "        w2 * row['CLOUD_AMT'] +\n",
    "        w3 * row['PRECTOTCORR']\n",
    "    )\n",
    "\n",
    "# Prepare a DataFrame to store the monthly scores\n",
    "monthly_scores = df[['lat', 'lon']].copy()\n",
    "\n",
    "# Loop through each month in 2020-2023 and calculate the suitability score \n",
    "for year in range(2020, 2024):\n",
    "    for month in range(1, 13):\n",
    "        ym = f\"{year}{month:02d}\"\n",
    "        try:\n",
    "            score = df.apply(lambda row: calculate_suitability({\n",
    "                'ALLSKY_SFC_SW_DWN': row[f'ALLSKY_SFC_SW_DWN_{ym}'],\n",
    "                'ALLSKY_KT': row[f'ALLSKY_KT_{ym}'],\n",
    "                'CLOUD_AMT': row[f'CLOUD_AMT_{ym}'],\n",
    "                'PRECTOTCORR': row[f'PRECTOTCORR_{ym}']\n",
    "            }), axis=1)\n",
    "            monthly_scores[f'score_{ym}'] = score.round(5)\n",
    "        except KeyError:\n",
    "            print(f\"Missing data for month {ym} ‚Äî skipping.\")\n",
    "\n",
    "# Identify the score columns (those that start with \"score_\")\n",
    "score_columns = [col for col in monthly_scores.columns if col.startswith(\"score_\")]\n",
    "\n",
    "# Calculate the average score across all 48 months for each point\n",
    "monthly_scores['avg_score'] = monthly_scores[score_columns].mean(axis=1).round(5)\n",
    "\n",
    "# Save the result\n",
    "output_path = \"monthly_suitability_scores_with_avg.csv\"\n",
    "monthly_scores.to_csv(output_path, index=False)\n",
    "\n",
    "output_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff988dc",
   "metadata": {},
   "source": [
    "## Extract point - score pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f497cc77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lat_lon_score.csv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"monthly_suitability_scores_with_avg.csv\")\n",
    "\n",
    "avg_score_df = monthly_scores[['lat', 'lon', 'avg_score']]\n",
    "\n",
    "# Save to a new CSV file\n",
    "output_path_avg_only = \"lat_lon_score.csv\" # This is the data that contains coordinates - score pairs and that will be passed to algos\n",
    "avg_score_df.to_csv(output_path_avg_only, index=False)\n",
    "\n",
    "output_path_avg_only\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
